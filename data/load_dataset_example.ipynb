{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of loading a dataset using the CustomImageDataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "\n",
    "from CustomDataset import CustomImageDataset\n",
    "from CustomDataset import augment_dataset as augment_dataset\n",
    "from CustomDataset import show_augmented_dataset_info as show_augmented_dataset_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATASET_DIR_SUFFIX = 'images'\n",
    "\n",
    "# Directory of the dataset\n",
    "dataset_type = 'clean_road'\n",
    "dataset_dir = os.path.join('../dataset', dataset_type, DATASET_DIR_SUFFIX)\n",
    "\n",
    "# Directory and file name of the labels\n",
    "label_dir = os.path.join('../dataset', dataset_type)\n",
    "label_file = label_dir+'/metadata.csv'\n",
    "\n",
    "# Output directory\n",
    "\n",
    "\n",
    "# Custom transformation\n",
    "#transform = T.Resize((256,256))\n",
    "transform = T.Compose([T.ToTensor(),])\n",
    "\n",
    "# Additional parameters for Dataloader\n",
    "batch_size = 4\n",
    "num_workers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset and load with Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset using CustomImageDataset\n",
    "dataset = CustomImageDataset(label_file, dataset_dir, transform, use_cv2=True)\n",
    "print(len(dataset))\n",
    "\n",
    "# Load the dataset using DataLoader\n",
    "data_batch = data.DataLoader(dataset, batch_size = batch_size, shuffle=True, num_workers=num_workers, drop_last=False)\n",
    "print(len(data_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first n_batches\n",
    "n_batches = 3\n",
    "\n",
    "for j, (imgs, labels) in enumerate(data_batch):\n",
    "  if j == n_batches:\n",
    "    break\n",
    "  print(imgs[\"name\"])\n",
    "  print(imgs[\"file\"].shape[0])\n",
    "  print(labels.shape)\n",
    "  print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create augmentation transform list\n",
    "augmentation_transform_list = []\n",
    "\n",
    "# Define AutoAugment augmentation transform\n",
    "augmentation_transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.AutoAugment(),\n",
    "    #T.RandAugment(),\n",
    "    #T.AugMix(),\n",
    "    #T.TrivialAugmentWide(),\n",
    "    T.ToTensor(),    \n",
    "])\n",
    "transform_name = \"AutoAugment\"\n",
    "\n",
    "# Populate augmentation_transform_list\n",
    "transform_dict = {\"name\": transform_name, \"transform\": augmentation_transform}\n",
    "augmentation_transform_list.append(transform_dict)\n",
    "\n",
    "# Apply augment_dataset function to create augmented dataset\n",
    "augmented_dataset = augment_dataset(dataset, augmentation_transform_list, create_dict=False)\n",
    "#augmented_dataset, augmentation_dictionary = augment_dataset(dataset, augmentation_transform_list, create_dict=True)\n",
    "\n",
    "print(len(augmented_dataset))\n",
    "\n",
    "# Load the augmented dataset using DataLoader\n",
    "augmented_batch = data.DataLoader(augmented_dataset, batch_size = batch_size, shuffle=True, num_workers=num_workers, drop_last=False)\n",
    "print(len(augmented_batch))\n",
    "\n",
    "show_augmented_dataset_info(augmented_dataset)\n",
    "#show_augmented_dataset_info(augmented_dataset, augmentation_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = dataset.split_train_test()\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
