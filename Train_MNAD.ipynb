{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNAD Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from data.CustomDataset import CustomImageDataset\n",
    "from data.CustomDataset import augment_dataset\n",
    "from data.CustomDataset import show_augmented_dataset_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR_SUFFIX = 'images'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "  \"gpus\": \"1\",                            # gpus (set 1 or None)\n",
    "  \"batch_size\": 4,                        # batch size for training\n",
    "  \"epochs\": 60,                           # number of epochs for training\n",
    "  \"loss_compact\": 0.1,                    # weight of the feature compactness loss\n",
    "  \"loss_separate\": 0.1,                   # weight of the feature separateness loss\n",
    "  \"h\": 256,                               # height of input images\n",
    "  \"w\": 256,                               # width of input images\n",
    "  \"c\": 3,                                 # channel of input images\n",
    "  \"lr\": 2e-4,                             # initial learning rate\n",
    "  \"method\": \"recon\",                      # The target task for anoamly detection (pred or recon)\n",
    "  \"t_length\": 1,                          # length of the frame sequences\n",
    "  \"fdim\": 512,                            # channel dimension of the features\n",
    "  \"mdim\": 512,                            # channel dimension of the memory items\n",
    "  \"msize\": 10,                            # number of the memory items\n",
    "  \"num_workers\": 2,                       # number of workers for the train loader\n",
    "  \"dataset_type\": \"clean_road\",           # type of dataset: clean_road\n",
    "  \"dataset_path\": \"./dataset\",            # directory of data\n",
    "  \"label_path\": \"./dataset\",              # directory of labels\n",
    "  \"label_file\": \"metadata.csv\",           # name of the label file\n",
    "  \"exp_dir\": \"./log\",                     # directory of log\n",
    "  \"split_dataset\": False                  # whether to split the dataset\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(**args_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "if args.gpus is not None and torch.cuda.is_available():\n",
    "\n",
    "  print(torch.cuda.device_count())\n",
    "\n",
    "  print(torch.cuda.current_device())\n",
    "\n",
    "  print(torch.cuda.device(0))\n",
    "\n",
    "  print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "if args.gpus is None:\n",
    "    gpus = \"0\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= gpus\n",
    "else:\n",
    "    gpus = \"\"\n",
    "    for i in range(len(args.gpus)):\n",
    "        gpus = gpus + args.gpus[i] + \",\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= gpus[:-1]\n",
    "\n",
    "#torch.backends.cudnn.enabled = True # make sure to use cudnn for computational performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = os.path.join(args.dataset_path, args.dataset_type, DATASET_DIR_SUFFIX)\n",
    "train_label_file = os.path.join(args.label_path, args.dataset_type, args.label_file)\n",
    "\n",
    "#transform = T.Resize((args.h,args.w))\n",
    "transform = T.Compose([T.ToTensor(),])\n",
    "\n",
    "# Create dataset\n",
    "dataset = CustomImageDataset(train_label_file, train_folder, transform = transform, use_cv2=True)\n",
    "dataset_size = len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.split_dataset:\n",
    "    train_dataset, _ = dataset.split_train_test()\n",
    "else:\n",
    "    train_dataset = dataset\n",
    "train_size = len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create augmentation transform list\n",
    "augmentation_transform_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AutoAugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enabled = True\n",
    "if enabled:\n",
    "    augmentation_transform = T.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.AutoAugment(),\n",
    "        T.ToTensor(),    \n",
    "    ])\n",
    "    transform_name = \"AutoAugment\"\n",
    "    applications_number = 3\n",
    "    transform_dict = {\"name\": transform_name, \"transform\": augmentation_transform, \"applications_number\": applications_number}\n",
    "    augmentation_transform_list.append(transform_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandAugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enabled = False\n",
    "if enabled:\n",
    "    augmentation_transform = T.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.RandAugment(),\n",
    "        T.ToTensor(),    \n",
    "    ])\n",
    "    transform_name = \"RandAugment\"\n",
    "    applications_number = 3\n",
    "    transform_dict = {\"name\": transform_name, \"transform\": augmentation_transform, \"applications_number\": applications_number}\n",
    "    augmentation_transform_list.append(transform_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AugMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enabled = False\n",
    "if enabled:\n",
    "    augmentation_transform = T.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.AugMix(),\n",
    "        T.ToTensor(),    \n",
    "    ])\n",
    "    transform_name = \"AugMix\"\n",
    "    applications_number = 3\n",
    "    transform_dict = {\"name\": transform_name, \"transform\": augmentation_transform, \"applications_number\": applications_number}\n",
    "    augmentation_transform_list.append(transform_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TrivialAgumentWide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enabled = False\n",
    "if enabled:\n",
    "    augmentation_transform = T.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.TrivialAugmentWide(),\n",
    "        T.ToTensor(),    \n",
    "    ])\n",
    "    transform_name = \"TrivialAugmentWide\"\n",
    "    applications_number = 3\n",
    "    transform_dict = {\"name\": transform_name, \"transform\": augmentation_transform, \"applications_number\": applications_number}\n",
    "    augmentation_transform_list.append(transform_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply augment_dataset function to create augmented dataset\n",
    "augmented_train_dataset = augment_dataset(train_dataset, augmentation_transform_list, create_dict=False)\n",
    "augmented_size = len(augmented_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "train_batch = data.DataLoader(augmented_train_dataset, batch_size = args.batch_size,\n",
    "                              shuffle=True, num_workers=args.num_workers, drop_last=True)\n",
    "batch_size = len(train_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_augmented_dataset_info(augmented_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setting\n",
    "assert args.method == 'pred' or args.method == 'recon', 'Wrong task name'\n",
    "if args.method == 'pred':\n",
    "    from model.final_future_prediction_with_memory_spatial_sumonly_weight_ranking_top1 import *\n",
    "    model = convAE(args.c, args.t_length, args.msize, args.fdim, args.mdim)\n",
    "else:\n",
    "    from model.Reconstruction import *\n",
    "    model = convAE(args.c, memory_size = args.msize, feature_dim = args.fdim, key_dim = args.mdim)\n",
    "params_encoder =  list(model.encoder.parameters())\n",
    "params_decoder = list(model.decoder.parameters())\n",
    "params = params_encoder + params_decoder\n",
    "optimizer = torch.optim.Adam(params, lr = args.lr)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max =args.epochs)\n",
    "\n",
    "if args.gpus is not None and torch.cuda.is_available():\n",
    "  model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set and create (if necessary) the log directory\n",
    "log_dir = os.path.join(args.exp_dir, args.dataset_type, args.method)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Write the augmentation list to a json file\n",
    "augmentations_file = open(os.path.join(log_dir, \"augmentations.json\"), \"w\")\n",
    "# Remove the transform objects from the dictionary\n",
    "for transform_dict in augmentation_transform_list:\n",
    "    transform_dict.pop(\"transform\")\n",
    "# Write the json file\n",
    "json.dump(augmentation_transform_list, augmentations_file)\n",
    "augmentations_file.close()\n",
    "\n",
    "# Set the log file\n",
    "orig_stdout = sys.stdout\n",
    "f = open(os.path.join(log_dir, 'log.txt'),'w')\n",
    "sys.stdout= f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss_func_mse = nn.MSELoss(reduction='none')\n",
    "# Initialize the memory items\n",
    "m_items = F.normalize(torch.rand((args.msize, args.mdim), dtype=torch.float), dim=1)\n",
    "if args.gpus is not None and torch.cuda.is_available():\n",
    "  m_items = m_items.cuda()\n",
    "\n",
    "# Training\n",
    "for epoch in range(args.epochs):\n",
    "    model.train()\n",
    "\n",
    "    for j,(images, labels) in enumerate(train_batch):\n",
    "\n",
    "        if args.gpus is not None and torch.cuda.is_available():\n",
    "          imgs = images[\"file\"].cuda()\n",
    "\n",
    "        if args.method == 'pred':\n",
    "            outputs, _, _, m_items, softmax_score_query, softmax_score_memory, separateness_loss, compactness_loss = model.forward(imgs[:,0:12], m_items, True)\n",
    "        else:\n",
    "            outputs, _, _, m_items, softmax_score_query, softmax_score_memory, separateness_loss, compactness_loss = model.forward(imgs, m_items, True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if args.method == 'pred':\n",
    "            loss_pixel = torch.mean(loss_func_mse(outputs, imgs[:,12:]))\n",
    "        else:\n",
    "            loss_pixel = torch.mean(loss_func_mse(outputs, imgs))\n",
    "\n",
    "        loss = loss_pixel + args.loss_compact * compactness_loss + args.loss_separate * separateness_loss\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print('----------------------------------------')\n",
    "    print('Epoch:', epoch+1)\n",
    "    if args.method == 'pred':\n",
    "        print('Loss: Prediction {:.6f}/ Compactness {:.6f}/ Separateness {:.6f}'.format(loss_pixel.item(), compactness_loss.item(), separateness_loss.item()))\n",
    "    else:\n",
    "        print('Loss: Reconstruction {:.6f}/ Compactness {:.6f}/ Separateness {:.6f}'.format(loss_pixel.item(), compactness_loss.item(), separateness_loss.item()))\n",
    "    print('Memory_items:')\n",
    "    print(m_items)\n",
    "    print('----------------------------------------')\n",
    "\n",
    "print('Training is finished')\n",
    "\n",
    "# Save the model and the memory items\n",
    "torch.save(model, os.path.join(log_dir, 'model.pth'))\n",
    "torch.save(m_items, os.path.join(log_dir, 'keys.pt'))\n",
    "\n",
    "sys.stdout = orig_stdout\n",
    "f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
