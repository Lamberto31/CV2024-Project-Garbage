{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNAD Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "\n",
    "from data.CustomDataset import CustomImageDataset\n",
    "from evaluation_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR_SUFFIX = 'images'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "  \"gpus\": \"1\",                            # gpus (set 1 or None)\n",
    "  \"batch_size\": 1,                        # batch size for testing\n",
    "  \"h\": 256,                               # height of input images\n",
    "  \"w\": 256,                               # width of input images\n",
    "  \"c\": 3,                                 # channel of input images\n",
    "  \"method\": \"recon\",                      # The target task for anoamly detection  (pred or recon)\n",
    "  \"t_length\": 1,                          # length of the frame sequences\n",
    "  \"fdim\": 512,                            # channel dimension of the features\n",
    "  \"mdim\": 512,                            # channel dimension of the memory items\n",
    "  \"msize\": 10,                            # number of the memory items\n",
    "  \"alpha\": 0.7,                           # weight for the anomality score\n",
    "  \"th\": 0.015,                            # threshold for test updating\n",
    "  \"num_workers\": 1,                       # number of workers for the test loader\n",
    "  \"dataset_type\": \"clean_road\",           # type of dataset: clean_road\n",
    "  \"dataset_path\": \"./dataset\",            # directory of data\n",
    "  \"label_path\": \"./dataset\",              # directory of labels\n",
    "  \"label_file\": \"test_labels.csv\",        # name of the label file\n",
    "  \"model_path\": \"model/trained\",          # directory of model\n",
    "  \"model_file\": \"model.pth\",              # name of the model file\n",
    "  \"m_items_path\": \"model/trained\",        # directory of memory items\n",
    "  \"m_items_file\": \"keys.pt\"               # name of the memory items file\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(**args_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "1\n",
      "0\n",
      "<torch.cuda.device object at 0x783436d543d0>\n",
      "Quadro T2000 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "if args.gpus is not None and torch.cuda.is_available():\n",
    "  print(torch.cuda.is_available())\n",
    "\n",
    "  print(torch.cuda.device_count())\n",
    "\n",
    "  print(torch.cuda.current_device())\n",
    "\n",
    "  print(torch.cuda.device(0))\n",
    "\n",
    "  print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "if args.gpus is None:\n",
    "    gpus = \"0\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= gpus\n",
    "else:\n",
    "    gpus = \"\"\n",
    "    for i in range(len(args.gpus)):\n",
    "        gpus = gpus + args.gpus[i] + \",\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= gpus[:-1]\n",
    "\n",
    "#torch.backends.cudnn.enabled = True # make sure to use cudnn for computational performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder = os.path.join(args.dataset_path, args.dataset_type, DATASET_DIR_SUFFIX)\n",
    "test_label_file = os.path.join(args.label_path, args.dataset_type, args.label_file)\n",
    "\n",
    "#transform = T.Resize((args.h,args.w))\n",
    "transform = T.Compose([T.ToTensor(),])\n",
    "\n",
    "# Loading dataset\n",
    "test_dataset = CustomImageDataset(test_label_file, test_folder, transform = transform, use_cv2=True)\n",
    "test_size = len(test_dataset)\n",
    "\n",
    "test_batch = data.DataLoader(test_dataset, batch_size = args.batch_size,\n",
    "                              shuffle=True, num_workers=args.num_workers, drop_last=False)\n",
    "batch_size = len(test_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the trained model\n",
    "model_file = os.path.join(args.model_path, args.dataset_type, args.model_file)\n",
    "m_items_file = os.path.join(args.m_items_path, args.dataset_type, args.m_items_file)\n",
    "#model = torch.load(args.model_dir, map_location=torch.device('cpu'))\n",
    "model = torch.load(model_file)\n",
    "model.cuda()\n",
    "#m_items = torch.load(args.m_items_dir, map_location=torch.device('cpu'))\n",
    "m_items = torch.load(m_items_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func_mse = nn.MSELoss(reduction='none')\n",
    "\n",
    "psnr_list = {}\n",
    "feature_distance_list = {}\n",
    "\n",
    "# Populate the dictionaries with the image names and empty lists\n",
    "for img_name in test_dataset.imgs_labels[\"filename\"].to_numpy():\n",
    "    psnr_list[img_name] = []\n",
    "    feature_distance_list[img_name] = []\n",
    "\n",
    "m_items_test = m_items.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of clean_road\n",
      "(73, 1)\n",
      "[[0.59364452]\n",
      " [0.44457223]\n",
      " [0.4220863 ]\n",
      " [0.52773352]\n",
      " [0.95523927]\n",
      " [0.46455824]\n",
      " [0.42038965]\n",
      " [0.57264705]\n",
      " [0.47618166]\n",
      " [0.59732785]\n",
      " [0.75484728]\n",
      " [0.39129666]\n",
      " [0.64701694]\n",
      " [0.75566816]\n",
      " [0.60088984]\n",
      " [0.23778584]\n",
      " [0.46126253]\n",
      " [0.74458321]\n",
      " [0.64864812]\n",
      " [0.37716803]\n",
      " [0.66398272]\n",
      " [0.58549444]\n",
      " [0.51925204]\n",
      " [0.85933824]\n",
      " [0.37157685]\n",
      " [0.30602963]\n",
      " [0.80176805]\n",
      " [0.55424223]\n",
      " [0.93456588]\n",
      " [0.        ]\n",
      " [0.49495765]\n",
      " [0.54696636]\n",
      " [0.54864325]\n",
      " [0.28679452]\n",
      " [0.28982122]\n",
      " [0.56100851]\n",
      " [0.83177449]\n",
      " [0.77087208]\n",
      " [0.60992194]\n",
      " [0.65712221]\n",
      " [0.31816818]\n",
      " [0.19919601]\n",
      " [0.56955331]\n",
      " [0.6450349 ]\n",
      " [0.95300313]\n",
      " [0.28992424]\n",
      " [0.56960272]\n",
      " [0.47048443]\n",
      " [0.5396862 ]\n",
      " [0.4732131 ]\n",
      " [0.75659953]\n",
      " [0.61850229]\n",
      " [0.51510994]\n",
      " [0.14154888]\n",
      " [0.69708666]\n",
      " [0.59990983]\n",
      " [0.38340262]\n",
      " [0.66253735]\n",
      " [0.56972592]\n",
      " [0.78451737]\n",
      " [0.53502412]\n",
      " [0.65140587]\n",
      " [0.05301594]\n",
      " [0.31847112]\n",
      " [0.52628148]\n",
      " [0.49007027]\n",
      " [0.42006926]\n",
      " [0.4398455 ]\n",
      " [0.5250926 ]\n",
      " [0.72554115]\n",
      " [0.77553904]\n",
      " [0.63174286]\n",
      " [0.31382678]]\n",
      "The result of  clean_road\n",
      "AUC:  93.40175953079178 %\n"
     ]
    }
   ],
   "source": [
    "print('Evaluation of', args.dataset_type)\n",
    "model.eval()\n",
    "\n",
    "for j,(images, labels) in enumerate(test_batch):\n",
    "    imgs = images[\"file\"]\n",
    "    img_name = images[\"name\"][0]\n",
    "\n",
    "    if args.gpus is not None and torch.cuda.is_available():\n",
    "        imgs = imgs.cuda()\n",
    "\n",
    "    if args.method == 'pred':\n",
    "        outputs, feas, updated_feas, m_items_test, softmax_score_query, softmax_score_memory, _, _, _, compactness_loss = model.forward(imgs[:,0:3*4], m_items_test, False)\n",
    "        mse_imgs = torch.mean(loss_func_mse((outputs[0]+1)/2, (imgs[0,3*4:]+1)/2)).item()\n",
    "        mse_feas = compactness_loss.item()\n",
    "\n",
    "        # Calculating the threshold for updating at the test time\n",
    "        point_sc = point_score(outputs, imgs[:,3*4:])\n",
    "\n",
    "    else:\n",
    "        outputs, feas, updated_feas, m_items_test, softmax_score_query, softmax_score_memory, compactness_loss = model.forward(imgs, m_items_test, False)\n",
    "        mse_imgs = torch.mean(loss_func_mse((outputs[0]+1)/2, (imgs[0]+1)/2)).item()\n",
    "        mse_feas = compactness_loss.item()\n",
    "\n",
    "        # Calculating the threshold for updating at the test time\n",
    "        point_sc = point_score(outputs, imgs)\n",
    "\n",
    "    if  point_sc < args.th:\n",
    "        query = nn.functional.normalize(feas, dim=1)\n",
    "        query = query.permute(0,2,3,1) # b X h X w X d\n",
    "        m_items_test = model.memory.update(query, m_items_test, False)\n",
    "    \n",
    "    psnr_list[img_name].append(psnr(mse_imgs))\n",
    "    feature_distance_list[img_name].append(mse_feas)\n",
    "\n",
    "# Measuring the abnormality score and the AUC\n",
    "anomaly_score_total_list = []\n",
    "\n",
    "# Calculating the abnormality score as the sum of the PSNR (inverted) and the feature distance\n",
    "psnr_listed = anomaly_score_list_inv(list(psnr_list.values()))\n",
    "feature_distance_listed = anomaly_score_list(list(feature_distance_list.values()))\n",
    "anomaly_score_total_list = score_sum(psnr_listed, feature_distance_listed, args.alpha)\n",
    "\n",
    "anomaly_score_total_list = np.asarray(anomaly_score_total_list)\n",
    "\n",
    "print(anomaly_score_total_list.shape)\n",
    "print(anomaly_score_total_list)\n",
    "\n",
    "# Calculating the AUC\n",
    "anomaly_score_total_list = np.expand_dims(anomaly_score_total_list, axis=0)\n",
    "labels_list = np.expand_dims(test_dataset.imgs_labels[\"label\"].to_numpy(), axis=0)\n",
    "\n",
    "accuracy = AUC(anomaly_score_total_list, labels_list)\n",
    "\n",
    "print('The result of ', args.dataset_type)\n",
    "print('AUC: ', accuracy*100, '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
